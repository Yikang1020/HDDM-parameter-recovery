{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de667e5-831a-4aaa-be05-76c37ce13bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/parallel.py:12: ShimWarning: The `IPython.parallel` package has been deprecated since IPython 4.0. You should import from ipyparallel instead.\n",
      "  warn(\"The `IPython.parallel` package has been deprecated since IPython 4.0. \"\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import estimate as est\n",
    "from copy import deepcopy, copy\n",
    "import numpy as np\n",
    "import hddm\n",
    "import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from generate_regression import *\n",
    "from estimate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9bb51736-163e-422a-bf3d-069d82d4e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def args():\n",
    "    \"\"\"\n",
    "    trials experiment\n",
    "    \"\"\"\n",
    "    run_type='regress'\n",
    "    estimators=['HDDMRegressor', 'SingleRegressor','MLRegressor']\n",
    "    n_subjs=12\n",
    "    n_trials=[20,30,40,50,75,100,150]\n",
    "    n_params=30\n",
    "    n_datasets=30\n",
    "    seed_data=1\n",
    "    seed_params=1\n",
    "    equal_seeds=True\n",
    "    include=['sv']\n",
    "    depends_on = {}\n",
    "    factor3_vals=[0.1, 0.3, 0.5]\n",
    "    subj_noise = OrderedDict([('v', 0.2), ('a', 0.2), ('t', 0.1)])\n",
    "    if 'z' in include:\n",
    "        subj_noise['z'] = .1\n",
    "    if 'sz' in include:\n",
    "        subj_noise['sz'] = .05\n",
    "    if 'st' in include:\n",
    "        subj_noise['st'] = .1\n",
    "    if 'sv' in include:\n",
    "        subj_noise['sv'] = .1\n",
    "    if run_type == 'regress':\n",
    "        subj_noise['v_inter'] = 0.1\n",
    "    return locals()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5df1f8a-9a0c-4531-a8fd-ea6cb0a58dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_NAMES = {'a': 'a',\n",
    "               'v': 'v',\n",
    "               't': 't',\n",
    "               'z': 'z',\n",
    "               'st': 'st',\n",
    "               'sz': 'sz',\n",
    "               'sv': 'sv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "135d33b0-c7ad-465c-b6d7-eebaa9770de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjs=(12,)\n",
    "n_trials=(10, 40, 100)\n",
    "n_params=5\n",
    "n_datasets=5\n",
    "include=('v','t','a')\n",
    "estimators=None\n",
    "view=None\n",
    "depends_on = None\n",
    "equal_seeds=True\n",
    "run_type=None\n",
    "factor3_vals = None\n",
    "action='run'\n",
    "single_runs_folder='.'\n",
    "subj_noise=None\n",
    "seed_data=1\n",
    "seed_params=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e190399-b4ab-4c84-92d5-3b79e7d5dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_type='regress'\n",
    "estimators=['HDDMRegressor', 'SingleRegressor','MLRegressor']\n",
    "n_subjs=12\n",
    "n_trials=[20,30,40,50,75,100,150]\n",
    "n_params=30\n",
    "n_datasets=30\n",
    "seed_data=1\n",
    "seed_params=1\n",
    "equal_seeds=True\n",
    "include=['sv']\n",
    "depends_on = {}\n",
    "factor3_vals=[0.1, 0.3, 0.5]\n",
    "subj_noise = OrderedDict([('v', 0.2), ('a', 0.2), ('t', 0.1)])\n",
    "if 'z' in include:\n",
    "    subj_noise['z'] = .1\n",
    "if 'sz' in include:\n",
    "    subj_noise['sz'] = .05\n",
    "if 'st' in include:\n",
    "    subj_noise['st'] = .1\n",
    "if 'sv' in include:\n",
    "    subj_noise['sv'] = .1\n",
    "if run_type == 'regress':\n",
    "    subj_noise['v_inter'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62c9085f-cfc2-4f67-b7d5-a60b2d06a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(n_subjs, (tuple, list, np.ndarray)):\n",
    "    n_subjs = (n_subjs,)\n",
    "if not isinstance(n_trials, (tuple, list, np.ndarray)):\n",
    "    n_trials = (n_trials,)\n",
    "if depends_on is None:\n",
    "    depends_on = {}\n",
    "#kwargs for initialize estimation\n",
    "init = OrderedDict([('include', include), ('depends_on', depends_on)])\n",
    "\n",
    "#kwargs for estimation\n",
    "estimate = OrderedDict([('runs', 3)])\n",
    "\n",
    "#include params\n",
    "params = OrderedDict([('include', include)])\n",
    "\n",
    "recover = est.multi_recovery_fixed_n_trials\n",
    "\n",
    "#estimator_dict\n",
    "estimator_dict = OrderedDict()\n",
    "\n",
    "#sampling params\n",
    "hddm_sampling_params = OrderedDict([('samples', 1500), \n",
    "                                    ('burn', 500), \n",
    "                                    ('map', False)])\n",
    "#optimizations params\n",
    "optimizations_params = OrderedDict([('method', 'ML'), \n",
    "                                    ('quantiles', (0.1, 0.3, 0.5, 0.7, 0.9)), \n",
    "                                    ('n_runs', 50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "485255e8-3ca5-497a-956f-51ecabde997f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sampling\n",
    "if 'SingleMAP' in estimators:\n",
    "    estimator_dict['SingleMAP'] = OrderedDict([('estimator', est.EstimationSingleMAP), \n",
    "                                               ('params', {'runs': 50})])\n",
    "\n",
    "if 'SingleMAPoutliers' in estimators:\n",
    "    estimator_dict['SingleMAPoutliers'] = OrderedDict([('estimator', est.EstimationSingleMAPoutliers), \n",
    "                                                       ('params', {'runs': 50})])\n",
    "\n",
    "if 'HDDMsharedVar' in estimators:\n",
    "    estimator_dict['HDDMsharedVar'] = OrderedDict([('estimator', est.EstimationHDDMsharedVar), \n",
    "                                                   ('params', hddm_sampling_params)])\n",
    "\n",
    "if 'HDDMGamma' in estimators:\n",
    "    estimator_dict['HDDMGamma'] = OrderedDict([('estimator', est.EstimationHDDMGamma), \n",
    "                                               ('params', hddm_sampling_params)])\n",
    "\n",
    "if 'noninformHDDM' in estimators:\n",
    "    estimator_dict['noninformHDDM'] = OrderedDict([('estimator', est.EstimationNoninformHDDM), \n",
    "                                                   ('params', hddm_sampling_params)])\n",
    "\n",
    "if 'HDDMOutliers' in estimators:\n",
    "    estimator_dict['HDDMOutliers'] = OrderedDict([('estimator', est.EstimationHDDMOutliers), \n",
    "                                                  ('params', hddm_sampling_params)])\n",
    "\n",
    "if 'HDDMRegressor' in estimators:\n",
    "    estimator_dict['HDDMRegressor'] = OrderedDict([('estimator', est.EstimationHDDMRegressor), \n",
    "                                                   ('params', hddm_sampling_params)])\n",
    "\n",
    "if 'HDDM2' in estimators:\n",
    "    estimator_dict['HDDM2'] = OrderedDict([('estimator', est.EstimationHDDM2), \n",
    "                                           ('params', hddm_sampling_params)])\n",
    "\n",
    "if 'SingleRegressor' in estimators:\n",
    "    estimator_dict['SingleRegressor'] = OrderedDict([('estimator', est.SingleRegressor), \n",
    "                                                     ('params', hddm_sampling_params)])\n",
    "\n",
    "if 'HDDM2Single' in estimators:\n",
    "    estimator_dict['HDDM2Single'] = OrderedDict([('estimator', est.EstimationHDDM2Single), \n",
    "                                                 ('params', hddm_sampling_params)])\n",
    "\n",
    "if 'HDDMTruncated' in estimators:\n",
    "    estimator_dict['HDDMTruncated'] = OrderedDict([('estimator', est.EstimationHDDMTruncated), \n",
    "                                                   ('params', hddm_sampling_params)])\n",
    "\n",
    "#optimization\n",
    "if 'Quantiles_subj' in estimators:\n",
    "    opt_params = deepcopy(optimizations_params)\n",
    "    opt_params['method'] = 'chisquare'\n",
    "    estimator_dict['Quantiles_subj'] = OrderedDict([('estimator', est.EstimationSingleOptimization), \n",
    "                                                    ('params', opt_params)])\n",
    "\n",
    "if 'ML' in estimators:\n",
    "    opt_params = deepcopy(optimizations_params)\n",
    "    opt_params['method'] = 'ML'\n",
    "    estimator_dict['ML'] = OrderedDict([('estimator', est.EstimationSingleOptimization), \n",
    "                                        ('params', opt_params)])\n",
    "\n",
    "if 'Quantiles_group' in estimators:\n",
    "    opt_params = deepcopy(optimizations_params)\n",
    "    opt_params['method'] = 'chisquare'\n",
    "    estimator_dict['Quantiles_group'] = OrderedDict([('estimator', est.EstimationGroupOptimization), \n",
    "                                                     ('params', opt_params)])\n",
    "\n",
    "if 'MLRegressor' in estimators:\n",
    "    opt_params = deepcopy(optimizations_params)\n",
    "    opt_params['method'] = 'ML'\n",
    "    estimator_dict['MLRegressor'] = OrderedDict([('estimator', est.SingleRegOptimization), \n",
    "                                                 ('params', opt_params)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d019b49-cd09-4d6a-a7f9-8bc916c0bff5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n",
      "Skiping job 123\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'include'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-f57dc5e27588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                 \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_conds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhddm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_rand_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkw_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'include'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v_slope'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meffect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                                 \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v_inter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'include'"
     ]
    }
   ],
   "source": [
    "n_subjs_results = {}\n",
    "for cur_subjs in n_subjs:\n",
    "    n_trials_results = {}\n",
    "    for cur_trials in n_trials:\n",
    "        factor3_results = {}\n",
    "        for cur_value in factor3_vals:\n",
    "            #if regress experiments then we add an effect\n",
    "            if run_type == 'regress':\n",
    "                effect = cur_value\n",
    "            \n",
    "                n_conds = 2     \n",
    "            #create kw_dict\n",
    "            kw_dict = OrderedDict([('params', params), \n",
    "                                   ('init', init), \n",
    "                                   ('estimate', estimate), \n",
    "                                   ('n_conds', n_conds)])\n",
    "            #exclude params\n",
    "            if run_type == 'regress':\n",
    "                exclude = set(['sv','st','sz','z', 'reg_outcomes'])\n",
    "            else:\n",
    "                exclude = set(['sv','st','sz','z']) - set(include)\n",
    "        \n",
    "            #create kw_dict['data']\n",
    "            if run_type == 'outliers':\n",
    "                cur_outliers = cur_value\n",
    "            else:\n",
    "                cur_outliers = 0\n",
    "            n_outliers = int(cur_trials * cur_outliers)\n",
    "            n_fast_outliers = (n_outliers // 2)\n",
    "            n_slow_outliers = n_outliers - n_fast_outliers\n",
    "            data = OrderedDict([('subjs', cur_subjs), ('subj_noise', subj_noise), ('size', cur_trials - n_outliers),\n",
    "                    ('exclude_params', exclude)])\n",
    "            \n",
    "            if run_type != 'regress':\n",
    "                    data['n_fast_outliers'] = n_fast_outliers\n",
    "                    data['n_slow_outliers'] = n_slow_outliers\n",
    "            #creat kw_dict\n",
    "            kw_dict['data'] = data\n",
    "            \n",
    "            models_results = {}\n",
    "            for model_name, descr in estimator_dict.items():\n",
    "                #create kw_dict\n",
    "                kw_dict_model = deepcopy(kw_dict)\n",
    "                kw_dict_model['estimate'] = descr['params']\n",
    "                \n",
    "                #update it with regressor information if needed\n",
    "                if model_name in est.MODELS_WITH_REGRESSORS:\n",
    "                    def reg_func(args, cols):\n",
    "                        return args[0]*cols[:,0]+args[1]\n",
    "                    if run_type == 'regress':\n",
    "                        reg = {'func': reg_func, 'args':['v_slope','v_inter'], 'covariates': 'cov', 'outcome':'v'}\n",
    "                    else:\n",
    "                        reg = {'func': reg_func, 'args':['v_shift','v(c0)'], 'covariates': 'condition', 'outcome':'v'}\n",
    "                    reg = OrderedDict(sorted(reg.items(), key=lambda t: t[0]))\n",
    "                    kw_dict_model['init']['regressor'] = reg\n",
    "                    kw_dict_model['init']['depends_on'] = {}\n",
    "                \n",
    "                #run analysis\n",
    "                estimation = descr['estimator'] \n",
    "                seed_data=seed_data \n",
    "                seed_params=seed_params \n",
    "                n_params=n_params\n",
    "                n_datasets=n_datasets \n",
    "                kw_dict=kw_dict_model \n",
    "                view=view\n",
    "                run_type=run_type\n",
    "                equal_seeds=equal_seeds\n",
    "                action=action \n",
    "                single_runs_folder=single_runs_folder\n",
    "\n",
    "                p_seeds = seed_params + np.arange(n_params)\n",
    "                d_seeds = seed_data + np.arange(n_datasets)\n",
    "                \n",
    "                p_results = {}\n",
    "                for p_seed in p_seeds:\n",
    "                    d_results = {}\n",
    "                    if equal_seeds:\n",
    "                        d_seeds = [p_seed]\n",
    "                    for d_seed in d_seeds:\n",
    "                        kw_seed = copy.deepcopy(kw_dict)\n",
    "                        kw_seed['seed_params'] = p_seed\n",
    "                        kw_seed['seed_data'] = d_seed\n",
    "                        \n",
    "                        kw_dict = kw_seed\n",
    "                        if view is None:\n",
    "                            #generate params and data for regression experiments\n",
    "                            n_conds = kw_dict['n_conds']\n",
    "                            if run_type == 'regress':\n",
    "                                np.random.seed(kw_dict['seed_params'])\n",
    "                                _ = kw_dict['params'].pop('n_conds', None)\n",
    "                                \n",
    "                                params = hddm.generate.gen_rand_params(kw_dict['params']['include'][0])\n",
    "                                params['v_slope'] = effect\n",
    "                                params['v_inter'] = 1\n",
    "                                params['sv'] = 0\n",
    "                                del params['v']\n",
    "\n",
    "                                params = params\n",
    "                                \n",
    "                                joined_params = params\n",
    "                            #generate params and data for other experiments\n",
    "                            else:\n",
    "                                np.random.seed(kw_dict['seed_params'])\n",
    "                                cond_v =  (np.random.rand()*0.4 + 0.1) * 2**np.arange(n_conds)\n",
    "                                params, joined_params = hddm.generate.gen_rand_params(cond_dict={'v':cond_v}, **kw_dict['params'])\n",
    "\n",
    "\n",
    "                            np.random.seed(kw_dict['seed_data'])\n",
    "\n",
    "                            #create a job hash\n",
    "                            kw_dict['estimator_class'] = estimation.__name__\n",
    "                            h = 123\n",
    "\n",
    "                            # check if job was already run, if so, load it!\n",
    "                            fname = os.path.join(single_runs_folder, '%s.dat' % str(h))\n",
    "                            if os.path.isfile(fname) and (action != 'rerun'):\n",
    "                                if action == 'collect':\n",
    "                                    stats = pd.read_pickle(fname)\n",
    "                                    print(\"Loading job %s\" % h)\n",
    "                                    run_estimation=False\n",
    "                                    if len(stats) == 0:\n",
    "                                        ouput =  stats\n",
    "                                elif action == 'run':\n",
    "                                    stats = pd.read_pickle(fname)\n",
    "                                    print(\"Skiping job %s\" % h)\n",
    "                                    ouput =  stats\n",
    "                                elif action == 'delete':\n",
    "                                    os.remove(fname)\n",
    "                                    ouput =  pd.DataFrame()\n",
    "                                else:\n",
    "                                    raise ValueError('Unknown action')\n",
    "\n",
    "                            else:\n",
    "                                #create a file that holds the results and to make sure that no other worker would start\n",
    "                                #working on this job\n",
    "                                pd.DataFrame().to_pickle(fname)\n",
    "\n",
    "                                #create a temporary file with a unique name\n",
    "                                temp_fname = fname + '.' + str(os.getpid())\n",
    "                                pd.DataFrame().to_pickle(temp_fname)\n",
    "\n",
    "                                #get list of files\n",
    "                                #if the length of the list is larger than one, then more than one worker is trying to perform the same job\n",
    "                                #in this case we leave only the job with the \"largest file name\"\n",
    "\n",
    "                                files = glob.glob(fname + '.*')\n",
    "                                #if we need to kill the job\n",
    "                                if (len(files) > 1) and (max(files) != temp_fname):\n",
    "                                    os.remove(temp_fname)\n",
    "                                    stats = pd.read_pickle(fname)\n",
    "                                    print(\"Loading job %s\" % h)\n",
    "                                    run_estimation=False\n",
    "                                    if len(stats) == 0:\n",
    "                                        ouput =  stats\n",
    "\n",
    "                                #else we will continue as usuall\n",
    "                                else:\n",
    "                                    print(\"Working on job %s (%s)\" % (h, estimation))\n",
    "                                    pprint.pprint(kw_dict)\n",
    "                                    run_estimation=True\n",
    "\n",
    "                            #generate params and data\n",
    "                            if run_type == 'regress':\n",
    "                                params['reg_outcomes'] = 'v'\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04710c-56c1-419f-bce2-a7c46f50d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                                data, group_params = gen_regression_data(params, **kw_dict['data'])\n",
    "                                group_params = {'c1': group_params}\n",
    "                                subj_noise = kw_dict['data']['subj_noise']\n",
    "                            else:\n",
    "                                data, group_params = hddm.generate.gen_rand_data(params, **kw_dict['data'])\n",
    "                                if kw_dict['data']['subjs'] == 1 and n_conds == 1:\n",
    "                                    group_params = {'c0': [group_params]}\n",
    "                                elif n_conds == 1:\n",
    "                                    group_params = {'c0': group_params}\n",
    "                                elif kw_dict['data']['subjs'] == 1:\n",
    "                                    for key, value in group_params.items():\n",
    "                                        group_params[key] = [value]\n",
    "                                subj_noise = kw_dict['data']['subj_noise']\n",
    "\n",
    "                                # prepare data for HDDMShift\n",
    "                                if estimation.__name__ in ESTIMATTIONS_WITH_REGRESSORS:\n",
    "                                    cond = np.zeros(len(data['condition']))\n",
    "                                    cond[data.condition == 'c1'] = 1\n",
    "                                    data['condition'] = cond\n",
    "\n",
    "                            if n_conds > 1:\n",
    "                                depends_on = {'v': 'condition'}\n",
    "                            else:\n",
    "                                depends_on = {}\n",
    "\n",
    "                            group_params = put_all_params_in_a_single_dict(joined_params, group_params, subj_noise, depends_on=depends_on)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5bfd51e-a477-4628-a56c-db55b4dd659c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sv': 0,\n",
       " 'sz': 0,\n",
       " 'st': 0,\n",
       " 'z': 0.5,\n",
       " 't': 0.3989143716007006,\n",
       " 'a': 0.74547608915413,\n",
       " 'v_slope': 0.1,\n",
       " 'v_inter': 1,\n",
       " 'reg_outcomes': 'v'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcff41b9-675f-412b-86e3-432672a4ab98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('subjs', 12),\n",
       "             ('subj_noise',\n",
       "              OrderedDict([('v', 0.2),\n",
       "                           ('a', 0.2),\n",
       "                           ('t', 0.1),\n",
       "                           ('sv', 0.1),\n",
       "                           ('v_inter', 0.1)])),\n",
       "             ('size', 20),\n",
       "             ('exclude_params', {'reg_outcomes', 'st', 'sv', 'sz', 'z'})])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_dict['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dbf1bfed-44bc-48ed-b855-ca7dfba828b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_params = kabuki.generate._add_noise({'none': params}, \n",
    "                           noise=OrderedDict([('v', 0.2),\n",
    "                           ('a', 0.2),\n",
    "                           ('t', 0.1),\n",
    "                           ('sv', 0.1),\n",
    "                           ('v_inter', 0.1)]), \n",
    "                           share_noise=share_noise,\n",
    "                                    check_valid_func=hddm.utils.check_params_valid,\n",
    "                                    bounds=bounds,\n",
    "                                    exclude_params={'reg_outcomes', 'st', 'sv', 'sz', 'z'})['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "82cd42d9-c948-4aa1-a9b8-7b7623be386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_params=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2529153e-044c-44c4-b272-438e7a1fbd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_params.append(subj_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a06b6de2-2985-485f-a243-eac5ad7a2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate v\n",
    "wfpt_params = deepcopy(subj_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3419f215-0a2f-4470-bfdf-ef5425d26c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sv': 0,\n",
       " 'sz': 0,\n",
       " 'st': 0,\n",
       " 'z': 0.5,\n",
       " 't': 0.3053374281747937,\n",
       " 'a': 0.6918984732289268,\n",
       " 'v_slope': 0.1,\n",
       " 'v_inter': 1.0530355466738186,\n",
       " 'reg_outcomes': 'v'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wfpt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6891bef4-8134-48f1-89d9-1e5a79363c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0530355466738186"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wfpt_params.pop('v_inter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "704d0d5a-2b9e-4329-b9e6-040af87c6af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "effect = wfpt_params.pop('v_slope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e55e66fe-fcbf-4822-a9c9-2d497ae4085c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sv': 0,\n",
       " 'sz': 0,\n",
       " 'st': 0,\n",
       " 'z': 0.5,\n",
       " 't': 0.3053374281747937,\n",
       " 'a': 0.6918984732289268,\n",
       " 'reg_outcomes': 'v'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wfpt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c49a92f6-b1c4-47c8-94a6-d90d870dc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.randn(20)\n",
    "x2 = np.random.randn(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c5e6e55e-cbe9-4e56-90a0-ccd2b72dc20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wfpt_params['v'] = (effect*x1 + np.sqrt(1-effect**2)*x2) + subj_params['v_inter'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "214c5a4e-7099-4730-a984-f9813666d857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(wfpt_params['v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ef13fc91-34e9-41f3-a05a-6841ed2f7148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sv': 0,\n",
       " 'sz': 0,\n",
       " 'st': 0,\n",
       " 'z': 0.5,\n",
       " 't': 0.3053374281747937,\n",
       " 'a': 0.6918984732289268,\n",
       " 'reg_outcomes': 'v',\n",
       " 'v': array([ 0.6202091 , -0.58662908,  0.27786649,  0.70308905,  2.01949274,\n",
       "         0.60618212,  1.30121571,  0.69131827,  1.22985234,  1.76741956,\n",
       "         1.71355584,  0.26107302, -0.1634425 ,  2.48044665,  1.81270213,\n",
       "         0.89509761,  0.23949069,  1.24567329,  0.27546498,  0.05210286])}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wfpt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "defd1e05-ba31-4c57-8850-b8dabdce1f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.373277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.521277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.383277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.365277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.441277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.349277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.383277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.437277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.591277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.431277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.491277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.329277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.349277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.503277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.353277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.395277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.343277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.415277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.379277</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.511277</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rt  response\n",
       "0   0.373277       1.0\n",
       "1   0.521277       0.0\n",
       "2   0.383277       1.0\n",
       "3   0.365277       1.0\n",
       "4   0.441277       1.0\n",
       "5   0.349277       0.0\n",
       "6   0.383277       0.0\n",
       "7   0.437277       0.0\n",
       "8   0.591277       1.0\n",
       "9   0.431277       1.0\n",
       "10  0.491277       1.0\n",
       "11  0.329277       0.0\n",
       "12  0.349277       0.0\n",
       "13  0.503277       1.0\n",
       "14  0.353277       0.0\n",
       "15  0.395277       1.0\n",
       "16  0.343277       1.0\n",
       "17  0.415277       1.0\n",
       "18  0.379277       0.0\n",
       "19  0.511277       1.0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_regression_rts(size=20, **wfpt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "847869f1-002e-4802-a02f-c977a5861a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_regression_rts(size, reg_outcomes, p_outlier=0, **params_dict):\n",
    "\n",
    "    i_params = deepcopy(params_dict)\n",
    "    sampled_rts = pd.DataFrame(np.zeros((size, 2)), columns=['rt', 'response'])\n",
    "    for i_sample in range(len(sampled_rts)):\n",
    "        #get current params\n",
    "        for p in reg_outcomes:\n",
    "            i_params[p] = params_dict[p][i_sample]\n",
    "        #sample\n",
    "        sampled_rts.iloc[i_sample,:] = hddm.generate.gen_rts(size=1, method='drift', dt=1e-3, **i_params).values\n",
    "    return sampled_rts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "07fc7b29-dea5-4cd3-9269-680d5adcef72",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gen_regression_rts() got multiple values for keyword argument 'reg_outcomes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-32c821103bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen_regression_rts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_outcomes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_outlier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwfpt_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: gen_regression_rts() got multiple values for keyword argument 'reg_outcomes'"
     ]
    }
   ],
   "source": [
    "gen_regression_rts(size=20, reg_outcomes=['v'], p_outlier=0, **wfpt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0ffaf29c-b754-4d5c-a6e8-d5e1058e8342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function kabuki.generate.gen_rand_data(gen_func, params, size=50, subjs=1, subj_noise=0.1, exclude_params=(), share_noise=(), column_name='data', check_valid_func=None, bounds=None, seed=None, generate_data=True)>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kabuki.generate.gen_rand_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c016679d-0049-4d96-9c05-21b066b3e595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(          rt  response  subj_idx condition\n",
       " 0   0.455277       1.0         0      none\n",
       " 1   0.411277       0.0         0      none\n",
       " 2   0.339277       1.0         0      none\n",
       " 3   0.513277       0.0         0      none\n",
       " 4   0.363277       0.0         0      none\n",
       " 5   0.571277       1.0         0      none\n",
       " 6   0.455277       1.0         0      none\n",
       " 7   0.383277       1.0         0      none\n",
       " 8   0.349277       1.0         0      none\n",
       " 9   0.349277       1.0         0      none\n",
       " 10  0.389277       1.0         0      none\n",
       " 11  0.459277       1.0         0      none\n",
       " 12  0.499277       0.0         0      none\n",
       " 13  0.367277       1.0         0      none\n",
       " 14  0.403277       0.0         0      none\n",
       " 15  0.441277       1.0         0      none\n",
       " 16  0.381277       1.0         0      none\n",
       " 17  0.387277       1.0         0      none\n",
       " 18  0.339277       0.0         0      none\n",
       " 19  0.461277       1.0         0      none,\n",
       " {'sv': 0,\n",
       "  'sz': 0,\n",
       "  'st': 0,\n",
       "  'z': 0.5,\n",
       "  't': 0.3053374281747937,\n",
       "  'a': 0.6918984732289268,\n",
       "  'reg_outcomes': 'v',\n",
       "  'v': array([ 0.6202091 , -0.58662908,  0.27786649,  0.70308905,  2.01949274,\n",
       "          0.60618212,  1.30121571,  0.69131827,  1.22985234,  1.76741956,\n",
       "          1.71355584,  0.26107302, -0.1634425 ,  2.48044665,  1.81270213,\n",
       "          0.89509761,  0.23949069,  1.24567329,  0.27546498,  0.05210286])})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kabuki.generate.gen_rand_data(gen_regression_rts, \n",
    "                              wfpt_params,\n",
    "                              size=20,\n",
    "                              check_valid_func=hddm.utils.check_params_valid,\n",
    "                              bounds=bounds, \n",
    "                              share_noise=share_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286994ee-547f-402a-a201-0a4fa0fbe66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_regression_data(params, subj_noise, share_noise = ('a','v','t','st','sz','sv','z', 'v_slope', 'v_inter'), size=50,\n",
    "                        subjs=1, exclude_params=(), **kwargs):\n",
    "\n",
    "    \"\"\"Generate simulated RTs with random parameters.\n",
    "\n",
    "       :Optional:\n",
    "            params : dict\n",
    "                Parameter names and values. If not\n",
    "                supplied, takes random values.\n",
    "            method : string\n",
    "                method to generate samples\n",
    "            the rest of the arguments are forwarded to kabuki.generate.gen_rand_data\n",
    "\n",
    "       :Returns:\n",
    "            data array with RTs\n",
    "            parameter values\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    from numpy import inf\n",
    "\n",
    "    # set valid param ranges\n",
    "    bounds = {'a': (0, inf),\n",
    "              'z': (0, 1),\n",
    "              't': (0, inf),\n",
    "              'st': (0, inf),\n",
    "              'sv': (0, inf),\n",
    "              'sz': (0, 1)\n",
    "    }\n",
    "\n",
    "\n",
    "    # Create RT data\n",
    "    group_params = []\n",
    "    for i_subj in range(subjs):\n",
    "      subj_params = kabuki.generate._add_noise({'none': params}, \n",
    "                                               noise=subj_noise, share_noise=share_noise,\n",
    "                                        check_valid_func=hddm.utils.check_params_valid,\n",
    "                                        bounds=bounds,\n",
    "                                        exclude_params=exclude_params)['none']\n",
    "      group_params.append(subj_params)\n",
    "\n",
    "      #generate v\n",
    "      wfpt_params = deepcopy(subj_params)\n",
    "      wfpt_params.pop('v_inter')\n",
    "      effect = wfpt_params.pop('v_slope')\n",
    "      x1 = np.random.randn(size);\n",
    "      x2 = np.random.randn(size);\n",
    "      wfpt_params['v'] = (effect*x1 + np.sqrt(1-effect**2)*x2) + subj_params['v_inter'];\n",
    "\n",
    "      #generate data\n",
    "      subj_data, _ = kabuki.generate.gen_rand_data(gen_regression_rts, wfpt_params,\n",
    "                                                        size=size,\n",
    "                                                        check_valid_func=hddm.utils.check_params_valid,\n",
    "                                                        bounds=bounds, share_noise=share_noise, **kwargs)\n",
    "\n",
    "      #fix data a little bit\n",
    "      subj_data['cov'] = x1\n",
    "      subj_data['subj_idx'] = i_subj\n",
    "\n",
    "      #concatante subj_data to group_data\n",
    "      if i_subj == 0:\n",
    "        data = subj_data\n",
    "      else:\n",
    "        data = pd.concat((data, subj_data), ignore_index=True)\n",
    "\n",
    "    return data, group_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b21e881c-2f75-47a8-8a01-c7b86a40ef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('subjs', 12),\n",
       "             ('subj_noise',\n",
       "              OrderedDict([('v', 0.2),\n",
       "                           ('a', 0.2),\n",
       "                           ('t', 0.1),\n",
       "                           ('sv', 0.1),\n",
       "                           ('v_inter', 0.1)])),\n",
       "             ('size', 20),\n",
       "             ('exclude_params', {'reg_outcomes', 'st', 'sv', 'sz', 'z'})])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed892a-99fc-4834-9170-7e24dfcd52f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26d515-707b-4fa8-a43f-d4fed592ce3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdde008-ae1a-464b-936b-dcf5cbfb51a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
